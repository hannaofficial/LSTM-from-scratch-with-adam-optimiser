{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e84da6b8-2f31-41c6-ad9b-7daf99cb0376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Data:\n",
    "    def __init__(self,path, seq_length):\n",
    "        self.fp = open(path,'r')\n",
    "        self.data = self.fp.read()\n",
    "        character = list(set(self.data))\n",
    "        self.char_to_index = { ch:i for (i,ch) in enumerate(character)}\n",
    "        self.index_to_char = { i:ch for (i,ch) in enumerate(character)}\n",
    "        self.data_size = len(self.data)\n",
    "        self.vocab_size = len(character)\n",
    "        self.tracker = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.tracker\n",
    "        input_end = self.tracker + self.seq_length\n",
    "        inputs = [self.char_to_index[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_index[ch] for ch in self.data[input_start+1 :input_end +1]]\n",
    "        self.tracker += self.seq_length\n",
    "        if self.tracker + self.seq_length >= self.data_size:\n",
    "            self.tracker = 0\n",
    "\n",
    "        return inputs, targets\n",
    "\n",
    "    def reset_tracker(self):\n",
    "        return self.tracker == 0\n",
    "\n",
    "    def close_file(self):\n",
    "        self.fp.close()\n",
    "\n",
    "\n",
    "def init_orthogonal(param):\n",
    "\n",
    "    if param.ndim < 2:\n",
    "        raise ValueError(\"Only parameters with 2 or more dimension are supported\")\n",
    "\n",
    "    rows, cols = param.shape\n",
    "    new_param  = np.random.randn(rows, cols)\n",
    "\n",
    "    if rows < cols:\n",
    "        new_param = new_param.T\n",
    "\n",
    "    q, r = np.linalg.qr(new_param)\n",
    "\n",
    "    d = np.diag(r,0)\n",
    "    ph = np.sign(d)\n",
    "    q *= ph\n",
    "\n",
    "    if rows < cols:\n",
    "        q = q.T\n",
    "    new_param = q\n",
    "    return new_param\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def init_lstm(hidden_size, vocab_size, z_size):\n",
    "\n",
    "    W_f = np.random.randn(hidden_size, z_size)\n",
    "    b_f = np.zeros((hidden_size, 1))\n",
    "\n",
    "    W_i = np.random.randn(hidden_size, z_size)\n",
    "    b_i = np.zeros((hidden_size, 1))\n",
    "\n",
    "    W_g = np.random.randn(hidden_size, z_size)\n",
    "    b_g = np.zeros((hidden_size, 1))\n",
    "\n",
    "    W_o =  np.random.randn(hidden_size, z_size)\n",
    "    b_o = np.zeros((hidden_size, 1))\n",
    "\n",
    "    W_v = np.random.randn(vocab_size, hidden_size)\n",
    "    b_v = np.zeros((vocab_size, 1))\n",
    "\n",
    "    W_i = init_orthogonal(W_i)\n",
    "    W_f = init_orthogonal(W_f)\n",
    "    W_g = init_orthogonal(W_g)\n",
    "    W_o = init_orthogonal(W_o)\n",
    "    W_v = init_orthogonal(W_v)\n",
    "\n",
    "    params =  W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v\n",
    "    return params\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "def one_hot_encode(input_idx,vocab_size):\n",
    "    one_hot_encode = np.zeros(vocab_size)\n",
    "    one_hot_encode[input_idx]=1\n",
    "    return one_hot_encode\n",
    "\n",
    "def one_hot_encode_sequence(provide_seq, vocab_size):\n",
    "    \n",
    "    encoding = np.array([one_hot_encode(idx,vocab_size) for idx in provide_seq])\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    return encoding\n",
    "    \n",
    "def clip_grad_norm(grads, max_norm=0.25):\n",
    "    max_norm = float(max_norm)\n",
    "    total_norm = 0\n",
    "\n",
    "    for grad in grads:\n",
    "        grad_norm = np.sum(np.power(grad , 2))\n",
    "        total_norm += grad_norm\n",
    "\n",
    "    total_norm = np.sqrt(total_norm)\n",
    "\n",
    "    clip_coff = max_norm/(total_norm + 1e-6)\n",
    "\n",
    "    if clip_coff < 1:\n",
    "        for grad in grads:\n",
    "            grad *= clip_coff\n",
    "\n",
    "    return grads\n",
    "\n",
    "\n",
    "def sigmoid(x, derivation=False):\n",
    "    \n",
    "    x += 1e-12\n",
    "    f = 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    if derivation: # Return the derivative of the function evaluated at x\n",
    "        return f * (1 - f)\n",
    "    else: # Return the forward pass of the function at x\n",
    "        return f\n",
    "\n",
    "def softmax(x, derivation=False):\n",
    "    x += 1e-12\n",
    "    f = np.exp(x)/ np.sum(np.exp(x))\n",
    "    if derivation:\n",
    "        pass\n",
    "    else:\n",
    "        return f\n",
    "\n",
    "def tanh(x, derivation = False):\n",
    "    x += 1e-12\n",
    "    f = (np.exp(x) - np.exp(-x))/(np.exp(x) + np.exp(-x))\n",
    "    if(derivation):\n",
    "        return 1 - f**2\n",
    "    else:\n",
    "        return f\n",
    "\n",
    "def adagrad_optimiser(params,grads,  sq_grad, learning_rate = 1e-4):\n",
    "\n",
    "    for param, grad, square_grad in zip(params, grads, sq_grad):\n",
    "        square_grad += np.power(grad,2)  #adding variance without weightage can hider the adagrad to converge to o error as variance increase the learning rate decrease and decrease and it can converse to zero alse that can effect on update of weight\n",
    "        param += -learning_rate*grad/np.sqrt(square_grad+1e-8)\n",
    "\n",
    "    return params\n",
    "\n",
    "\n",
    "def adam_optimiser(params, grads, sq_grad,moment_grad, learning_rate = 1e-4):\n",
    "     beta = 0.9\n",
    "     beta_2 = 0.999\n",
    "     t=1 \n",
    "     for param, grad, variance_grad, m_grad in zip(params, grads, sq_grad,moment_grad):\n",
    "         variance_grad = beta_2*variance_grad + (1-beta_2)*np.power(grad,2) \n",
    "         v_hat = variance_grad / (1 - beta_2**t)\n",
    "         m_grad = beta*m_grad + (1 - beta)*grad\n",
    "         m_hat = m_grad / (1 - beta**t)\n",
    "         param = param - m_hat*(learning_rate/(np.sqrt(v_hat) + 1e-8))  # below is actually root over of variace i.e. standard deviation it actually adjust the learning rate base on s.d\n",
    "         t+=1\n",
    "\n",
    "\n",
    "     return params\n",
    "\n",
    "\n",
    "#forward logic\n",
    "\n",
    "def forward(inputs, h_prev, C_prev, param):\n",
    "\n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = param\n",
    "\n",
    "    x_s, z_s, f_s, i_s = [], [], [], []\n",
    "    g_s, C_s, o_s, h_s = [], [], [], []\n",
    "    v_s, output_s = [], []\n",
    "\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "\n",
    "\n",
    "    for x in inputs:\n",
    "\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "\n",
    "        f = sigmoid(np.dot(W_f,z) + b_f)\n",
    "        f_s.append(f)\n",
    "\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "\n",
    "        g = np.tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "\n",
    "        C_prev = f*C_prev + i*g\n",
    "        C_s.append(C_prev)\n",
    "\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "\n",
    "        h_prev = o*np.tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        \n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "\n",
    "        output = softmax(v)\n",
    "        output_s.append(output)\n",
    "\n",
    "    return z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, output_s \n",
    "\n",
    "#backward logic\n",
    "\n",
    "def backward(z, f, i, g, C, o, h, v, outputs, targets, p ):\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = p\n",
    "    \n",
    "    W_f_d = np.zeros_like(W_f)\n",
    "    b_f_d = np.zeros_like(b_f)\n",
    "\n",
    "    W_i_d = np.zeros_like(W_i)\n",
    "    b_i_d = np.zeros_like(b_i)\n",
    "\n",
    "    W_g_d = np.zeros_like(W_g)\n",
    "    b_g_d = np.zeros_like(b_g)\n",
    "\n",
    "    W_o_d = np.zeros_like(W_o)\n",
    "    b_o_d = np.zeros_like(b_o)\n",
    "\n",
    "    W_v_d = np.zeros_like(W_v)\n",
    "    b_v_d = np.zeros_like(b_v)\n",
    "\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    dC_next = np.zeros_like(C[0])\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for t in reversed(range(len(outputs))):\n",
    "\n",
    "            loss += -np.mean(np.log(outputs[t])*targets[t])   #output is actually softmax value\n",
    "            C_prev = C[t-1]\n",
    "    \n",
    "            dv = np.copy(outputs[t])\n",
    "            dv[np.argmax(targets[t])] -= 1\n",
    "    \n",
    "            W_v_d += np.dot(dv, h[t].T)\n",
    "            b_v_d += dv\n",
    "    \n",
    "            dh = np.dot(W_v.T, dv)\n",
    "            dh += dh_next\n",
    "            do = dh*np.tanh(C[t])\n",
    "            do *= sigmoid(o[t], derivation=True)\n",
    "    \n",
    "            W_o_d += np.dot(do, z[t].T)\n",
    "            b_o_d += do\n",
    "    \n",
    "            dC = np.copy(dC_next)\n",
    "            dC += dh*o[t]*tanh(C[t], derivation = True)\n",
    "            dg = dC*i[t]\n",
    "            dg *= tanh(g[t], derivation=True)\n",
    "    \n",
    "            W_g_d += np.dot(dg, z[t].T)\n",
    "            b_g_d += dg\n",
    "    \n",
    "            di = dC*g[t]\n",
    "            di *= sigmoid(i[t], derivation=True)\n",
    "            W_i_d += np.dot(di, z[t].T)\n",
    "            b_i_d += di\n",
    "    \n",
    "            df = dC*C_prev\n",
    "            df *= sigmoid(f[t], derivation=True)\n",
    "            W_f_d += np.dot(df, z[t].T)\n",
    "            b_f_d += df\n",
    "    \n",
    "            dz = (np.dot(W_f.T, df) + np.dot(W_i.T , di) + np.dot(W_g.T, dg) + np.dot(W_o.T, do))\n",
    "            dh_prev = dz[:hidden_size, :]\n",
    "            dC_prev = f[t]*dC\n",
    "    \n",
    "    grads= W_f_d, W_i_d, W_g_d, W_o_d, W_v_d, b_f_d, b_i_d, b_g_d, b_o_d, b_v_d\n",
    "\n",
    "    grads = clip_grad_norm(grads)\n",
    "    return loss, grads\n",
    "               \n",
    "def sample(h_prev,C_prev,params, inputs,hidden_size, n, temperature=1.0):\n",
    "    \n",
    "    assert h_prev.shape == (hidden_size, 1)\n",
    "    assert C_prev.shape == (hidden_size, 1)\n",
    "    inputs = inputs.reshape((vocab_len, 1))\n",
    "\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = params\n",
    "\n",
    "    x_s, z_s, f_s, i_s = [], [], [], []\n",
    "    g_s, C_s, o_s, h_s = [], [], [], []\n",
    "    v_s, output_s = [], []\n",
    "\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "\n",
    "\n",
    "    for t in range(n):\n",
    "\n",
    "        z = np.row_stack((h_prev, inputs))\n",
    "        z_s.append(z)\n",
    "\n",
    "        f = sigmoid(np.dot(W_f,z) + b_f)\n",
    "        f_s.append(f)\n",
    "\n",
    "        i = sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "\n",
    "        g = np.tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "\n",
    "        C_prev = f*C_prev + i*g\n",
    "        C_s.append(C_prev)\n",
    "\n",
    "        o = sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "\n",
    "        h_prev = o*np.tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        \n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "\n",
    "        output = softmax(v/temperature)\n",
    "        index = np.random.choice(range(vocab_len), p=output.ravel())\n",
    "        inputs = np.zeros((vocab_len, 1))\n",
    "        inputs[index] = 1\n",
    "        output_s.append(index)\n",
    "    return output_s        \n",
    "\n",
    "\n",
    "def train_LSTM(data_reader, hidden_size,params, sq_grad, moment_grad):  #I tried to put temperature but it didn't work\n",
    "    iter_num = 0\n",
    "    threshold = 0.01\n",
    "\n",
    "    smooth_loss = -np.log(1/data_reader.vocab_size)*data_reader.seq_length\n",
    "    while (smooth_loss > threshold):\n",
    "        if data_reader.reset_tracker():\n",
    "            h = np.zeros((hidden_size, 1))\n",
    "            c = np.zeros((hidden_size, 1))\n",
    "\n",
    "        inputs, targets = data_reader.next_batch()\n",
    "        inputs_one_hot = one_hot_encode_sequence(inputs, data_reader.vocab_size)\n",
    "        targets_one_hot  = one_hot_encode_sequence(targets, data_reader.vocab_size)\n",
    "\n",
    "        z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs = forward(inputs_one_hot, h, c, params)\n",
    "        loss, grads = backward(z_s, f_s, i_s, g_s, C_s, o_s, h_s, v_s, outputs, targets_one_hot, params)\n",
    "        params = adam_optimiser(params, grads,sq_grad, moment_grad)\n",
    "        smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "        h = h_s[-1]\n",
    "        c = C_s[-1]\n",
    "        if not iter_num%500:\n",
    "            # , temperature = initial_temperature*(0.999**(initial_temperature/100)) this temperature is used to concenterated the probability distribution to more repetared word\n",
    "            sample_ix = sample(h, c, params, inputs_one_hot[0],hidden_size,n=100 )\n",
    "            sample_txt = ''.join(data_reader.index_to_char[ix] for ix in sample_ix)\n",
    "            print(\"\")\n",
    "            print(f\"Iteration: {iter_num}  Loss: {smooth_loss:.4f}\")\n",
    "            print(f\"Sample: {sample_txt}\")\n",
    "\n",
    "        iter_num += 1\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94bfe1a-db82-423c-a6a2-5410d04f3241",
   "metadata": {},
   "source": [
    "**Here we will take data from input.txt where shaks data is there then we will train the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9758c4a8-c904-4c22-bc7d-4a33f7d2f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = Data('input.txt',seq_length=40)\n",
    "vocab_len = data_reader.vocab_size\n",
    "hidden_size = 512\n",
    "z_size = vocab_len + hidden_size\n",
    "params  = init_lstm(hidden_size, vocab_len, z_size) \n",
    "moment_grad = [np.zeros_like(p) for p in params]\n",
    "sq_grad =    [np.zeros_like(p) for p in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf920f3-c81a-4306-a87e-bfda5c734142",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_LSTM(data_reader,hidden_size, params, sq_grad, moment_grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
